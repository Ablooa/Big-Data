{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c716f98-5409-4719-8325-5722bb9afecb",
   "metadata": {},
   "source": [
    "# Блок 1. Standalone Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a021ea20-b43c-45fd-8a59-06a4cf72e427",
   "metadata": {},
   "source": [
    "### 1) Развернуть standalone cluster Spark: master + 2 workers. Приложить скрипт и/или алгоритм + скрин webui (10 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd1f5eb-169d-4762-9ece-63c7edee3db2",
   "metadata": {},
   "source": [
    "Скрины webui и файл docker-compose.yml в репозитории task_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2983e3ad-a689-4a14-80e6-37ba507b397c",
   "metadata": {},
   "source": [
    "### 2) Подключиться к кластеру с помощью Jupyter и/или Zeppelin. Приложить скрипт и/или алгоритм + скрин рабочей сессии из инструмента (20 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310858eb-52d4-4ac7-86de-a5228f4803b6",
   "metadata": {},
   "source": [
    "Файл docker-compose.yml для подключения Jupyter также в репозитории task_1\n",
    "\n",
    "Я сделала один файл для всего, чтобы одной командой все сразу запустить можно было"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55d2c831-7579-4d33-9ec0-5a31eb3eb145",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://33121fabf24a:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f988c776bc0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"spark\").master(\"spark://spark-master:7077\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17536b32-6251-41bd-bf8d-ddae14a20376",
   "metadata": {},
   "source": [
    "### 3) + 10 дополнительных баллов за развертывание и подключение к HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663c7f8-c4a7-45d0-8e0f-97867392086b",
   "metadata": {},
   "source": [
    "В прошлом ДЗ уже развертывали подключение к кластеру hadoop c hdfs. Поэтому для подтверждения все файлы из задания 2 были перенесены сюда, этому отобразим их:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42a63b76-139c-48f5-ab69-e28419d59fc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book1-100k.csv, book1000k-1100k.csv, book100k-200k.csv, book1100k-1200k.csv, book1200k-1300k.csv, book1300k-1400k.csv, book1400k-1500k.csv, book1500k-1600k.csv, book1600k-1700k.csv, book1700k-1800k.csv, book1800k-1900k.csv, book1900k-2000k.csv, book2000k-3000k.csv, book200k-300k.csv, book3000k-4000k.csv, book300k-400k.csv, book4000k-5000k.csv, book400k-500k.csv, book500k-600k.csv, book600k-700k.csv, book700k-800k.csv, book800k-900k.csv, book900k-1000k.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(f\"http://hadoop-namenode:9870/webhdfs/v1/upload?op=LISTSTATUS\").json()\n",
    "\n",
    "if response.get(\"FileStatuses\"):\n",
    "    files_data = response[\"FileStatuses\"].get(\"FileStatus\")\n",
    "    books = [file.get(\"pathSuffix\") for file in files_data if 'book' in file.get(\"pathSuffix\")]\n",
    "print(*books, sep = ', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb52572-763f-4ae4-954d-494fc04ec81a",
   "metadata": {},
   "source": [
    "# Блок 2. Работа с данными на Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27363d12-7c93-429c-af44-6f7b36301043",
   "metadata": {},
   "source": [
    "### 1) Преобразовать данные исходного датасета в parquet объединяя все таблицы. Оценить разницу в скорости чтения / занимаемом объеме. Сделать выводы. (15 баллов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a390a4f5-f99e-4978-9bc0-c8f500943fc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Определяем схему данных, используя первую строку файла\n",
    "schema = spark.read.csv(f\"hdfs://hadoop-namenode:9000/upload/{books[0]}\", header=True, inferSchema=True).limit(1).schema\n",
    "\n",
    "# Создаем пустой DataFrame с заданной схемой\n",
    "spark_df = spark.createDataFrame(data = [], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18fb9dca-4d6e-4b7f-b53c-974f10cc03d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Добавляем данные по книгам в spark_data\n",
    "for file in books:\n",
    "    spark_data = (\n",
    "        spark.read\n",
    "        .option(\"multiline\", \"true\")\n",
    "        .option(\"quote\", '\"')\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"escape\", \"\\\\\")\n",
    "        .option(\"escape\", '\"')\n",
    "        .csv(f\"hdfs://hadoop-namenode:9000/upload/{file}\")\n",
    "    )\n",
    "    spark_df = spark_df.unionByName(spark_data, allowMissingColumns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d82ba792-5c84-4d6a-82ad-70f29ef5de8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Записываем данные в нужном формате\n",
    "write = spark_df.write \\\n",
    "              .option(header=True) \\\n",
    "              .mode(\"overwrite\") \\\n",
    "              .parquet(path=\"hdfs://hadoop-namenode:9000/book.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07fa63f3-7bcc-464d-b6f9-f78c365da179",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Считываем их через spark.read.parquet\n",
    "spark_df = spark.read.parquet(f\"hdfs://hadoop-namenode:9000/book.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b5d7c5b2-c36a-4283-9518-2e12656c030f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время загрузки данных из CSV:  24.131360054016113\n",
      "Время загрузки данных из Parquet:  0.32906532287597656\n",
      "Размер файла CSV:  1110.6515398025513\n",
      "Размер файла Parquet:  692.3238620758057\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "# Загрузка исходных данных\n",
    "start_time_csv = time.time()\n",
    "df_csv = spark.read.csv(f\"hdfs://hadoop-namenode:9000/upload/*.csv\", header=True, inferSchema=True)\n",
    "end_time_csv = time.time()\n",
    "\n",
    "# Загрузка данных в формате Parquet\n",
    "start_time_parquet = time.time()\n",
    "df_parquet = spark.read.parquet(f\"hdfs://hadoop-namenode:9000/book.parquet\")\n",
    "end_time_parquet = time.time()\n",
    "\n",
    "# Сравнение времени загрузки файла\n",
    "print(\"Время загрузки данных из CSV: \", end_time_csv - start_time_csv)\n",
    "print(\"Время загрузки данных из Parquet: \", end_time_parquet - start_time_parquet)\n",
    "\n",
    "# Сравнение размера файла\n",
    "response_csv = requests.get(f\"http://hadoop-namenode:9000/webhdfs/v1/upload?op=LISTSTATUS\").json()\n",
    "if response_csv.get(\"FileStatuses\"):\n",
    "    files_data = response_csv[\"FileStatuses\"].get(\"FileStatus\")\n",
    "    files_size = [int(file.get(\"length\")) for file in files_data if \"book\" in file.get(\"pathSuffix\")]\n",
    "print(\"Размер файла CSV: \", sum(files_size) / 1024**2)\n",
    "\n",
    "response_parquet = requests.get(f\"http://hadoop-namenode:9000/webhdfs/v1/book.parquet?op=LISTSTATUS\").json()\n",
    "if response_parquet.get(\"FileStatuses\"):\n",
    "    files_data = response_parquet[\"FileStatuses\"].get(\"FileStatus\")\n",
    "    files_size = [int(file.get(\"length\")) for file in files_data if \"\" in file.get(\"pathSuffix\")]\n",
    "print(\"Размер файла Parquet: \", sum(files_size) / 1024**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08e5c32-9b28-4840-8425-c6658ea3ad2f",
   "metadata": {},
   "source": [
    "#### Видно, что данные, преобразованные в parquet занимают в 2 раза меньше памяти и времени на их загрузку также необходимо намного меньше."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ba7547-f412-44cd-9283-d7e28c66b032",
   "metadata": {},
   "source": [
    "### 2) Используя весь набор данных с помощью Spark вывести (5 баллов за каждое задание)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95869555-ed66-46c9-92cd-87a92bc8c338",
   "metadata": {},
   "source": [
    "#### a) Топ-10 книг с наибольшим числом ревью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "406f318f-b36d-439f-912c-d495b3864a5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 91:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+--------------+\n",
      "|Name                                                     |CountsOfReview|\n",
      "+---------------------------------------------------------+--------------+\n",
      "|The Hunger Games (The Hunger Games, #1)                  |154447        |\n",
      "|Twilight (Twilight, #1)                                  |94850         |\n",
      "|The Book Thief                                           |87685         |\n",
      "|The Help                                                 |76040         |\n",
      "|Harry Potter and the Sorcerer's Stone (Harry Potter, #1) |75911         |\n",
      "|The Giver (The Giver, #1)                                |57034         |\n",
      "|Water for Elephants                                      |52918         |\n",
      "|The Girl with the Dragon Tattoo (Millennium, #1)         |52225         |\n",
      "|Harry Potter and the Deathly Hallows (Harry Potter, #7)  |52088         |\n",
      "|The Lightning Thief (Percy Jackson and the Olympians, #1)|48630         |\n",
      "+---------------------------------------------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Перевод в тип integer\n",
    "spark_df = spark_df.withColumn(\"CountsOfReview\", spark_df.CountsOfReview.cast(IntegerType()))\n",
    "\n",
    "# Сортируем значения колонки по убыванию\n",
    "top_review_books = spark_df.select(\"Name\", \"CountsOfReview\").orderBy(spark_df.CountsOfReview.desc())\n",
    "\n",
    "# Выбираем топ 10\n",
    "top_review_books.limit(10).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ad75f-cc51-4d02-a518-da004fad67a8",
   "metadata": {},
   "source": [
    "#### b) Топ-10 издателей с наибольшим средним числом страниц в книгах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5612a80-a4a1-477a-b95c-635d7112af9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------+------------------+\n",
      "|Publisher                                                  |avg_pages         |\n",
      "+-----------------------------------------------------------+------------------+\n",
      "|Crafty Secrets Publications                                |1807321.6         |\n",
      "|Sacred-texts.com                                           |500000.0          |\n",
      "|Department of Russian Language and Literature University of|322128.5714285714 |\n",
      "|Logos Research Systems                                     |100000.0          |\n",
      "|Encyclopedia Britannica, Incorporated                      |32642.0           |\n",
      "|Progressive Management                                     |19106.3625        |\n",
      "|Still Waters Revival Books                                 |10080.142857142857|\n",
      "|P. Shalom Publications, Incorporated                       |8539.0            |\n",
      "|Hendrickson Publishers, Inc. (Peabody, MA)                 |6448.0            |\n",
      "|IEEE/EMB                                                   |6000.0            |\n",
      "+-----------------------------------------------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Группируем по издателям, считаем среднее кол-во страниц\n",
    "top_publishers = spark_df.groupBy(\"Publisher\").agg(avg(\"pagesNumber\").alias(\"avg_pages\"))\n",
    "\n",
    "# Выбираем топ 10\n",
    "top_publishers = spark_df.orderBy(\"avg_pages\", ascending=False).limit(10)\n",
    "top_publishers.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f4ab86-884d-4316-bb46-ff8d6087a17d",
   "metadata": {},
   "source": [
    "#### c) Десять наиболее активных по числу изданных книг лет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "346db3f0-f335-4513-a5b9-df7d59bc68e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|PublishYear|count |\n",
      "+-----------+------+\n",
      "|2007       |129503|\n",
      "|2006       |122363|\n",
      "|2005       |117630|\n",
      "|2004       |105727|\n",
      "|2003       |104341|\n",
      "|2002       |95531 |\n",
      "|2001       |88224 |\n",
      "|2000       |87286 |\n",
      "|2008       |80265 |\n",
      "|1999       |80153 |\n",
      "+-----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, desc\n",
    "\n",
    "# Убираем пустые значения из Года публикации и груупируя по этой колонке считаем кол-во строчек\n",
    "top_publish_years = spark_df.filter(col(\"PublishYear\").isNotNull()).groupBy(\"PublishYear\").count()\n",
    "\n",
    "# Выбираем топ 10\n",
    "top_publish_years = spark_df.orderBy(desc(\"count\")).limit(10)\n",
    "top_publish_years.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7207a973-0643-4c31-b4d7-c0df532294de",
   "metadata": {},
   "source": [
    "#### d) Топ-10 книг имеющих наибольший разброс в оценках среди книг имеющих больше 500 оценок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "19037846-e7f1-4cc5-949e-cd92a12f8e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 91:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+\n",
      "|     Id|                Name|            stddev|\n",
      "+-------+--------------------+------------------+\n",
      "|4593339|Ο Χάρι Πότερ και ...|1884480.9762179346|\n",
      "|3529641|Harry Potter og D...| 1857551.557211347|\n",
      "|3484606|Harry Potter e a ...|1856879.5144942228|\n",
      "|3102821|Harry Potter and ...|1850987.2240634726|\n",
      "|2200812|Harry Potter och ...|1833126.7167265879|\n",
      "|1990311|Harry Potter och ...|1824451.7246133152|\n",
      "|1907452|Harry Potter e a ...|1822858.0094387494|\n",
      "|1885731|Harry Potter and ...|1815587.3930200385|\n",
      "|1811146|Harry Potter i Ka...|1814183.3459608485|\n",
      "|1726635|Harri Potter maen...|1811594.8417798886|\n",
      "+-------+--------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col, split, array, explode, stddev_samp\n",
    "\n",
    "filtered_books = (spark_df\n",
    "                  .select('Id', 'Name', 'RatingDist1', 'RatingDist2', 'RatingDist3', 'RatingDist4', 'RatingDist5', 'RatingDistTotal')\n",
    "                  \n",
    "                  # Извлекаем только число из колонки RatingDistTotal\n",
    "                  .withColumn('RatingDistTotal', regexp_extract('RatingDistTotal', r'(\\d+)', 1).cast('int'))\n",
    "                  \n",
    "                   # Отфильтровываем строки, которые имеют более 500 оценок\n",
    "                  .filter(col('RatingDistTotal') > 500)\n",
    "                  \n",
    "                  # Создаем новую колонку, объединяющую все оценки в массив\n",
    "                  .withColumn('RatingDist', array(*[split(col(c), ':').getItem(1).cast('int') for c in ['RatingDist1', 'RatingDist2', 'RatingDist3', 'RatingDist4', 'RatingDist5']]))\n",
    "                  \n",
    "                  # Разворачиваем массив в отдельные строки\n",
    "                  .select('Id', 'Name', explode('RatingDist').alias('Rating'))\n",
    "                  .groupBy('Id', 'Name')\n",
    "                  \n",
    "                  # Рассчитываем стандартное отклонение по каждой строке\n",
    "                  .agg(stddev_samp('Rating').alias('stddev'))\n",
    "                  \n",
    "                  # Сортируем по убыванию стандартного отклонения и выбираем топ 10\n",
    "                  .orderBy(col('stddev').desc())\n",
    "                  .limit(10)\n",
    ")\n",
    "\n",
    "filtered_books.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63eaaa8",
   "metadata": {},
   "source": [
    "Специально нашла данные по первым двум книгам из этого странного результирующего списка, чтобы убедиться в правильности расчетов и вручную посчитала std:\n",
    "\n",
    "ID 4593339\n",
    "Данные по оценкам: 5:4608992\t4:1621963\t3:603633\t2:140565\t1:119534\n",
    "\n",
    "ID 3529641\n",
    "Данные по оценкам: 5:4543188\t4:1600072\t3:596615\t2:138407\t1:117311\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e9b4bff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для ID 4593339 стандартное отклонение = 1884480.9762179346\n",
      "Для ID 3529641 стандартное отклонение = 1857551.557211347\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "data_1 = {5: 4608992, 4: 1621963, 3: 603633, 2: 140565, 1: 119534}\n",
    "data_2 = {5:4543188, 4:1600072, 3:596615, 2:138407, 1:117311}\n",
    "std_dev_1 = statistics.stdev(data_1.values())\n",
    "std_dev_2 = statistics.stdev(data_2.values())\n",
    "print(f'Для ID 4593339 стандартное отклонение = {std_dev_1}')\n",
    "print(f'Для ID 3529641 стандартное отклонение = {std_dev_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435828af",
   "metadata": {},
   "source": [
    "С расчетами выше совпало, так что будем считать с технической стороны все правильно. На счет выбросов, не думаю, что тут есть смысл их смотреть, так как именно в этих фичах разброс слишком большой и неочевидный для идентификации выбросов.\n",
    "\n",
    "Да и весь Гирри Поттер иначе может улететь, уж слишком он крут."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4c45cd-560a-491d-9a79-42a1f230da28",
   "metadata": {},
   "source": [
    "#### e) Любой интересный инсайт из данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644d7839-e4d8-4fb5-a50d-e3df3f8f4571",
   "metadata": {},
   "source": [
    "#### Средний рейтинг авторов с наибольшим числом книг\n",
    "\n",
    "Значения null и выбросы (значения больше 10 и меньше 0) были проигнорированы в расчете общего числа книг, так как интересовал именно средний рейтинг этих книг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "df5d55f0-ca2d-4e27-931e-8af00fd27c96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 95:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+----------+\n",
      "|             Authors|        avg_rating|book_count|\n",
      "+--------------------+------------------+----------+\n",
      "|           Anonymous| 4.260478767693589|      2402|\n",
      "| William Shakespeare|3.8776038575667644|      1348|\n",
      "|             Unknown| 3.753079268292683|       984|\n",
      "|     Francine Pascal|3.5652150537634415|       930|\n",
      "|     Agatha Christie|3.8879661016949147|       885|\n",
      "|Fodor's Travel Pu...|3.7324803149606307|       762|\n",
      "|        Harold Bloom|3.8629234972677593|       732|\n",
      "|        Isaac Asimov| 3.986965317919076|       692|\n",
      "|       Carolyn Keene| 3.777635239567233|       647|\n",
      "|        Nora Roberts|3.9833384853168465|       647|\n",
      "+--------------------+------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc, avg, count, col\n",
    "\n",
    "\n",
    "top_authors = (spark_df.select('Authors', 'Rating')\n",
    "               \n",
    "                       # Фильтруем значения от 0 до 10, убирая выбросы\n",
    "                      .filter(col('Rating').isNotNull() & (col('Rating') > 0) & (col('Rating') <= 10))\n",
    "               \n",
    "                       # Группируем по автору и считаем средний рейтинг\n",
    "                      .groupBy('Authors') \n",
    "                      .agg(avg('Rating').alias('avg_rating'), count('*').alias('book_count')) )\n",
    "\n",
    "# Сортируем по убыванию количества книг и выбираем топ 10\n",
    "top_authors = top_authors.orderBy(desc('book_count')).limit(10)\n",
    "\n",
    "top_authors.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85af436-7696-47df-a559-aeeef62994ad",
   "metadata": {},
   "source": [
    "# Блок 3. Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ab77cc-7ee9-4832-8463-b7f36a8a9b55",
   "metadata": {},
   "source": [
    "В задании предлагается реализовать расчет среднего рейтинга книги на Spark Streaming со\n",
    "следующими условиями (30 баллов):\n",
    "- использовать данные user_rating как ﬁle source\n",
    "- использовать ﬁle sink в формате parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f49d6231-e153-4f32-91ba-74daa181d522",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, avg\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1d8b6d3e-939c-47c5-8d83-d718b8b57f25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_rating_0_to_1000.csv, user_rating_1000_to_2000.csv, user_rating_2000_to_3000.csv, user_rating_3000_to_4000.csv, user_rating_4000_to_5000.csv, user_rating_5000_to_6000.csv, user_rating_6000_to_11000.csv\n"
     ]
    }
   ],
   "source": [
    "# Рядом с данными book также лежат данные user_rating, отобразим их\n",
    "if response.get(\"FileStatuses\"):\n",
    "    files_data = response[\"FileStatuses\"].get(\"FileStatus\")\n",
    "    users = [file.get(\"pathSuffix\") for file in files_data if 'user_rating' in file.get(\"pathSuffix\")]\n",
    "print(*users, sep = ', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5f4ef24b-ad5f-4496-a85f-0d4f8de9259e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Определяем схему данных\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", StringType()),\n",
    "    StructField(\"Name\", StringType()),\n",
    "    StructField(\"Rating\", StringType()),\n",
    "    StructField(\"timestamp\", TimestampType())\n",
    "])\n",
    "\n",
    "# Читаем все файлы из папки\n",
    "streamingDataFrame = None\n",
    "for file in users:\n",
    "    if streamingDataFrame is None:\n",
    "        # Если это первый файл, то создаем потоковый DataFrame\n",
    "        streamingDataFrame = (\n",
    "            spark.readStream\n",
    "            .option(\"header\", True)\n",
    "            .option(\"escape\", \"\\\\\")\n",
    "            .option(\"maxFilesPerTrigger\", 1)\n",
    "            .option(\"escape\", '\"')\n",
    "            .schema(schema)\n",
    "            .csv(f\"hdfs://hadoop-namenode:9000/upload/{file}\")\n",
    "        )\n",
    "    else:\n",
    "        # Иначе добавляем следующий DataFrame в уже существующий потоковый DataFrame\n",
    "        nextDataFrame = (\n",
    "            spark.readStream\n",
    "            .option(\"header\", True)\n",
    "            .option(\"escape\", \"\\\\\")\n",
    "            .option(\"maxFilesPerTrigger\", 1)\n",
    "            .option(\"escape\", '\"')\n",
    "            .schema(schema)\n",
    "            .csv(f\"hdfs://hadoop-namenode:9000/upload/{file}\")\n",
    "        )\n",
    "        streamingDataFrame = streamingDataFrame.union(nextDataFrame)\n",
    "\n",
    "# Создаем новую колонку с числовыми значениями рейтинга\n",
    "spark_df = streamingDataFrame.withColumn('NumericalRating', when(col('Rating') == 'did not like it', 1)\\\n",
    "                                           .when(col('Rating') == 'it was ok', 2)\\\n",
    "                                           .when(col('Rating') == 'liked it', 3)\\\n",
    "                                           .when(col('Rating') == 'really liked it', 4)\\\n",
    "                                           .when(col('Rating') == 'it was amazing', 5)\\\n",
    "                                           .otherwise(None))\n",
    "\n",
    "# Игнорируем строки со значением \"This user doesn't have any rating\"\n",
    "spark_df = spark_df.filter(col('NumericalRating').isNotNull())\n",
    "\n",
    "# Добавляем watermark на основе временной метки\n",
    "spark_df_with_watermark = spark_df.withWatermark(\"timestamp\", \"10 minutes\")\n",
    "\n",
    "# Считаем среднее значение новой колонки по книгам\n",
    "avg_rating_by_book = spark_df_with_watermark \\\n",
    "                     .groupBy('Name', window('timestamp', '10 minutes')) \\\n",
    "                     .agg({'NumericalRating':'avg'})\n",
    "\n",
    "\n",
    "# Создаем Query\n",
    "query = (avg_rating_by_book\n",
    "         .writeStream\n",
    "         .format(\"parquet\")\n",
    "         .option(\"checkpointLocation\", \"hdfs://hadoop-namenode:9000/checkpoints\")\n",
    "         .option(\"path\", \"hdfs://hadoop-namenode:9000/output\")\n",
    "         .outputMode(\"append\")\n",
    "         .start()\n",
    "         .awaitTermination())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb60b91-495f-4a3b-96bc-bd17c9b13725",
   "metadata": {},
   "source": [
    "На всякий случай, оставлю отдельно то, что выведет код запроса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "00c7c51e-3633-4abf-bd26-516b02472d62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 114:================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                Name|avg(NumericalRating)|\n",
      "+--------------------+--------------------+\n",
      "|    Babar and Zephir|                 3.5|\n",
      "|      The Blue Aspic|                 3.0|\n",
      "|      Doctor Faustus|   2.761904761904762|\n",
      "|The Girl Who Play...|                3.75|\n",
      "|The Church of Sol...|                 3.0|\n",
      "|The Dream Life of...|  3.3333333333333335|\n",
      "|90 Below: Or, Wha...|                 3.0|\n",
      "|The Vice Guide to...|  2.3333333333333335|\n",
      "|Eleanor Oliphant ...|  3.2698412698412698|\n",
      "|Dawn of Wonder (T...|                 2.5|\n",
      "|Witches Abroad (D...|               3.125|\n",
      "|Sourcery (Discwor...|              3.0625|\n",
      "|The Moonshot Effe...|                 1.0|\n",
      "|La Gloire de mon ...|                 4.0|\n",
      "|    Only Revolutions|                 2.0|\n",
      "|The Berlin Storie...|  3.2222222222222223|\n",
      "|Career of Evil (C...|   2.851063829787234|\n",
      "|Before I Go to Sleep|  2.4054054054054053|\n",
      "|The Jefferson Bib...|                 3.0|\n",
      "|Principles of Con...|                 3.0|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Создаем новую колонку с числовыми значениями рейтинга\n",
    "spark_df = spark_df.withColumn('NumericalRating', when(col('Rating') == 'did not like it', 1)\\\n",
    "    .when(col('Rating') == 'it was ok', 2)\\\n",
    "    .when(col('Rating') == 'liked it', 3)\\\n",
    "    .when(col('Rating') == 'really liked it', 4)\\\n",
    "    .when(col('Rating') == 'it was amazing', 5)\\\n",
    "    .otherwise(None))\n",
    "\n",
    "# Игнорируем строки со значением \"This user doesn't have any rating\"\n",
    "spark_df = spark_df.filter(col('NumericalRating').isNotNull())\n",
    "\n",
    "# Считаем среднее значение новой колонки по книгам\n",
    "avg_rating_by_book = spark_df.groupBy('Name').agg({'NumericalRating':'avg'})\n",
    "avg_rating_by_book.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5387feea-9bd8-4ebb-aec6-a5d5493c67d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
